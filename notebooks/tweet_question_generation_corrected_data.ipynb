{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBKGoeg9KM5k"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4617,
     "status": "ok",
     "timestamp": 1598087154140,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "aweZgxXBDsOQ",
    "outputId": "cd0d0302-81b4-4de6-929f-7935c98e5a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers==3.0.0 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.7)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.1.91)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4752,
     "status": "ok",
     "timestamp": 1598087157186,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "4FARi6xuQ4IZ",
    "outputId": "5c2e8d62-1839-437a-8c97-6f66f2c10ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6946,
     "status": "ok",
     "timestamp": 1598087161051,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "iMjtG0q-Ws9B",
    "outputId": "55bc3c82-b245-43be-8733-9020e1122207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlp in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from nlp) (2.0.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (1.0.5)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (1.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->nlp) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13246,
     "status": "ok",
     "timestamp": 1598087174856,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "kvV3hytU59ab",
    "outputId": "833ec8be-23a8-4102-97c9-3060d08769ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Maluuba/nlg-eval.git@master\n",
      "  Cloning https://github.com/Maluuba/nlg-eval.git (to revision master) to /tmp/pip-req-build-ql1r64u3\n",
      "  Running command git clone -q https://github.com/Maluuba/nlg-eval.git /tmp/pip-req-build-ql1r64u3\n",
      "Requirement already satisfied (use --upgrade to upgrade): nlg-eval==2.3 from git+https://github.com/Maluuba/nlg-eval.git@master in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: click>=6.3 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (7.1.2)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (3.2.5)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.18.5)\n",
      "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (5.7.2)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (2.23.0)\n",
      "Requirement already satisfied: six>=1.11 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (0.22.2.post1)\n",
      "Requirement already satisfied: gensim>=3 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (3.6.0)\n",
      "Requirement already satisfied: Theano>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.0.5)\n",
      "Requirement already satisfied: tqdm>=4.24 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (4.41.1)\n",
      "Requirement already satisfied: xdg in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (4.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (2020.6.20)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17->nlg-eval==2.3) (0.16.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3->nlg-eval==2.3) (2.1.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (1.14.37)\n",
      "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (2.49.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (1.17.37)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.10.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (2.8.1)\n",
      "Building wheels for collected packages: nlg-eval\n",
      "  Building wheel for nlg-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nlg-eval: filename=nlg_eval-2.3-cp36-none-any.whl size=68175138 sha256=d4bceb33ce44b1062eb98002f95238c29595278e8f514710080644c9785af7d7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_vwnh7yb/wheels/a5/7c/fd/f312beca2adcc3f49cb40570730658dad37bb5709f5d237a56\n",
      "Successfully built nlg-eval\n"
     ]
    }
   ],
   "source": [
    "!pip3 install git+https://github.com/Maluuba/nlg-eval.git@master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12961,
     "status": "ok",
     "timestamp": 1598087177381,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "MRHG89D_6RPm",
    "outputId": "ef7ac83c-5647-4209-97f0-73a1d65e2159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\u001b[31mInstalling to /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[31mIn case of incomplete downloads, delete the directory and run `nlg-eval --setup /root/.cache/nlgeval' again.\u001b[0m\n",
      "Downloading https://raw.githubusercontent.com/robmsmt/glove-gensim/4c2224bccd61627b76c50a5e1d6afd1c82699d22/glove2word2vec.py to /usr/local/lib/python3.6/dist-packages/nlgeval/word2vec.\n",
      "glove2word2vec.py: 100% 1.00/1.00 [00:00<00:00, 616 chunks/s]\n"
     ]
    }
   ],
   "source": [
    "!nlg-eval --setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19343,
     "status": "ok",
     "timestamp": 1598087310079,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "-vXCbg-5ImHI",
    "outputId": "137eea56-4914-4399-f26b-604eae8d1b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2528,
     "status": "ok",
     "timestamp": 1598087310083,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "oy_T1CiVVNuH",
    "outputId": "b0453290-e63a-4886-a90f-7212fa8e1af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/question_generation/question_generation\n"
     ]
    }
   ],
   "source": [
    "%cd drive/My\\ Drive/question_generation/question_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iUcNtTKyIjK3",
    "outputId": "a2352c8d-5971-4691-8b58-ef184dce0781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_dev_squad_manual_edit.json  tweet_train_squad_manual_edit.json\n",
      "tweet_manual_multitask.py\n"
     ]
    }
   ],
   "source": [
    "ls data/tweet_manual_multitask/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6Okz9rhOWhiP",
    "outputId": "18a14109-992a-4ec5-9bd0-1d760cd06fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-20 19:05:37.584306: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "08/20/2020 19:05:39 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
      "08/20/2020 19:05:39 - INFO - transformers.tokenization_utils -   Adding <sep> to the vocabulary\n",
      "08/20/2020 19:05:39 - INFO - transformers.tokenization_utils -   Adding <hl> to the vocabulary\n",
      "08/20/2020 19:05:39 - INFO - nlp.load -   Checking data/tweet_manual_multitask/tweet_manual_multitask.py for additional imports.\n",
      "08/20/2020 19:05:39 - INFO - filelock -   Lock 140323353226040 acquired on data/tweet_manual_multitask/tweet_manual_multitask.py.lock\n",
      "08/20/2020 19:05:39 - INFO - nlp.load -   Found main folder for dataset data/tweet_manual_multitask/tweet_manual_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_manual_multitask\n",
      "08/20/2020 19:05:39 - INFO - nlp.load -   Creating specific version folder for dataset data/tweet_manual_multitask/tweet_manual_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_manual_multitask/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d\n",
      "08/20/2020 19:05:39 - INFO - nlp.load -   Copying script file from data/tweet_manual_multitask/tweet_manual_multitask.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_manual_multitask/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tweet_manual_multitask.py\n",
      "08/20/2020 19:05:39 - INFO - nlp.load -   Couldn't find dataset infos file at data/tweet_manual_multitask/dataset_infos.json\n",
      "08/20/2020 19:05:39 - INFO - nlp.load -   Creating metadata file for dataset data/tweet_manual_multitask/tweet_manual_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_manual_multitask/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tweet_manual_multitask.json\n",
      "08/20/2020 19:05:39 - INFO - filelock -   Lock 140323353226040 released on data/tweet_manual_multitask/tweet_manual_multitask.py.lock\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "08/20/2020 19:05:39 - INFO - nlp.builder -   Generating dataset squad_multitask (/root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d)\n",
      "Downloading and preparing dataset squad_multitask/highlight_qg_format (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d...\n",
      "08/20/2020 19:05:40 - INFO - nlp.builder -   Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "08/20/2020 19:05:40 - INFO - nlp.utils.info_utils -   Unable to verify checksums.\n",
      "08/20/2020 19:05:40 - INFO - nlp.builder -   Generating split train\n",
      "0 examples [00:00, ? examples/s]08/20/2020 19:05:40 - INFO - root -   generating examples from = /content/drive/My Drive/question_generation/question_generation/data/tweet_manual_multitask/tweet_train_squad_manual_edit.json\n",
      "08/20/2020 19:05:42 - INFO - nlp.arrow_writer -   Done writing 41507 examples in 10296027 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d.incomplete/squad_multitask-train.arrow.\n",
      "08/20/2020 19:05:42 - INFO - nlp.builder -   Generating split validation\n",
      "0 examples [00:00, ? examples/s]08/20/2020 19:05:42 - INFO - root -   generating examples from = /content/drive/My Drive/question_generation/question_generation/data/tweet_manual_multitask/tweet_dev_squad_manual_edit.json\n",
      "08/20/2020 19:05:42 - INFO - nlp.arrow_writer -   Done writing 3469 examples in 880816 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d.incomplete/squad_multitask-validation.arrow.\n",
      "08/20/2020 19:05:42 - INFO - nlp.utils.info_utils -   Unable to verify splits sizes.\n",
      "Dataset squad_multitask downloaded and prepared to /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d. Subsequent calls will reuse this data.\n",
      "08/20/2020 19:05:42 - INFO - nlp.builder -   Constructing Dataset for split train, from /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d\n",
      "08/20/2020 19:05:42 - INFO - nlp.utils.info_utils -   Unable to verify checksums.\n",
      "08/20/2020 19:05:42 - INFO - nlp.load -   Checking data/tweet_manual_multitask/tweet_manual_multitask.py for additional imports.\n",
      "08/20/2020 19:05:42 - INFO - filelock -   Lock 140323353196472 acquired on data/tweet_manual_multitask/tweet_manual_multitask.py.lock\n",
      "08/20/2020 19:05:42 - INFO - nlp.load -   Found main folder for dataset data/tweet_manual_multitask/tweet_manual_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_manual_multitask\n",
      "08/20/2020 19:05:42 - INFO - nlp.load -   Found specific version folder for dataset data/tweet_manual_multitask/tweet_manual_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_manual_multitask/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d\n",
      "08/20/2020 19:05:42 - INFO - nlp.load -   Found script file from data/tweet_manual_multitask/tweet_manual_multitask.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_manual_multitask/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tweet_manual_multitask.py\n",
      "08/20/2020 19:05:42 - INFO - nlp.load -   Couldn't find dataset infos file at data/tweet_manual_multitask/dataset_infos.json\n",
      "08/20/2020 19:05:42 - INFO - nlp.load -   Found metadata file for dataset data/tweet_manual_multitask/tweet_manual_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_manual_multitask/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tweet_manual_multitask.json\n",
      "08/20/2020 19:05:42 - INFO - filelock -   Lock 140323353196472 released on data/tweet_manual_multitask/tweet_manual_multitask.py.lock\n",
      "08/20/2020 19:05:42 - INFO - nlp.builder -   Overwrite dataset info from restored data version.\n",
      "08/20/2020 19:05:42 - INFO - nlp.info -   Loading Dataset info from /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d\n",
      "08/20/2020 19:05:42 - INFO - nlp.builder -   Reusing dataset squad_multitask (/root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d)\n",
      "08/20/2020 19:05:42 - INFO - nlp.builder -   Constructing Dataset for split validation, from /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d\n",
      "08/20/2020 19:05:43 - INFO - nlp.utils.info_utils -   Unable to verify checksums.\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/cache-385f8db752e9b46ea6fe2a445e7b5453.arrow\n",
      "100% 42/42 [00:00<00:00, 159.14it/s]\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_writer -   Done writing 10691 examples in 2848933 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tmpphximduv.\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/cache-c964c7ea09b11bbc7f3cfe6fdba16451.arrow\n",
      "100% 4/4 [00:00<00:00, 185.02it/s]\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_writer -   Done writing 1085 examples in 277865 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tmp7sre9ob8.\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/cache-1bde36e2c6adacfafd4a1912196beb6c.arrow\n",
      "100% 10691/10691 [00:00<00:00, 35968.66it/s]\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_writer -   Done writing 10691 examples in 2955471 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tmpleb82ip9.\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/cache-768a25c198123fc7c827b05d9d581491.arrow\n",
      "100% 10691/10691 [00:00<00:00, 35161.27it/s]\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_writer -   Done writing 10691 examples in 2827179 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tmpkwzfpmlx.\n",
      "08/20/2020 19:05:43 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/cache-3f5feb2034af54d5401b0a93ae99378a.arrow\n",
      "100% 11/11 [00:20<00:00,  1.83s/it]\n",
      "08/20/2020 19:06:03 - INFO - nlp.arrow_writer -   Done writing 10691 examples in 93273171 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tmpto1bytbt.\n",
      "08/20/2020 19:06:03 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/cache-650cdd2eba7b638d930d545ddc47079c.arrow\n",
      "100% 1085/1085 [00:00<00:00, 34153.78it/s]\n",
      "08/20/2020 19:06:04 - INFO - nlp.arrow_writer -   Done writing 1085 examples in 288691 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tmpyepojtld.\n",
      "08/20/2020 19:06:04 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/cache-d9c7ed0bbdf0e3593d0ef443dd95d48e.arrow\n",
      "100% 1085/1085 [00:00<00:00, 34618.05it/s]\n",
      "08/20/2020 19:06:04 - INFO - nlp.arrow_writer -   Done writing 1085 examples in 275671 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tmpgg1hhp_5.\n",
      "08/20/2020 19:06:04 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/cache-d2c57ee0b1d706e7a387ef1fcc90a88a.arrow\n",
      "100% 2/2 [00:02<00:00,  1.03s/it]\n",
      "08/20/2020 19:06:06 - INFO - nlp.arrow_writer -   Done writing 1085 examples in 9454795 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/a5b4dc1174978fb27b04d14e1b35abd5f90bd94bfbbf6f9889e7a84626ebce6d/tmpg5f30ox6.\n",
      "08/20/2020 19:06:06 - INFO - nlp.arrow_dataset -   Set __getitem__(key) output type to torch for ['source_ids', 'target_ids', 'attention_mask'] columns  (when key is int or slice) and don't output other (un-formated) columns.\n",
      "08/20/2020 19:06:06 - INFO - nlp.arrow_dataset -   Set __getitem__(key) output type to torch for ['source_ids', 'target_ids', 'attention_mask'] columns  (when key is int or slice) and don't output other (un-formated) columns.\n",
      "08/20/2020 19:06:06 - INFO - __main__ -   saved train dataset at data/train_data_qg_hl_t5_tweet_manual.pt\n",
      "08/20/2020 19:06:06 - INFO - __main__ -   saved validation dataset at data/valid_data_qg_hl_t5_tweet_manual.pt\n",
      "08/20/2020 19:06:08 - INFO - __main__ -   saved tokenizer at t5_qg_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Already DONE !!!!!\n",
    "#!python3 prepare_data.py --task qg --model_type t5 --dataset_path data/tweet_manual_multitask --qg_format highlight_qg_format --max_source_length 512 --max_target_length 32 --train_file_name train_data_qg_hl_t5_tweet_manual.pt  --valid_file_name valid_data_qg_hl_t5_tweet_manual.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI57MBkuRNPm"
   },
   "outputs": [],
   "source": [
    "###################### \n",
    "# valhalla/t5-small-qg-hl\n",
    "######################\n",
    "#08/15/2020 15:57:21 - INFO - transformers.configuration_utils -   Configuration saved in t5-small-qg-hl/config.json\n",
    "#08/15/2020 15:57:23 - INFO - transformers.modeling_utils -   Model weights saved in t5-small-qg-hl/pytorch_model.bin\n",
    "%%capture loggingfine\n",
    "!python3 run_qg.py \\\n",
    "    --model_name_or_path valhalla/t5-small-qg-hl \\\n",
    "    --model_type t5 \\\n",
    "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
    "    --output_dir t5-small-qg-hl-15-manual \\\n",
    "    --train_file_path data/train_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --seed 42 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100\\\n",
    "    --logging_dir t5-small-qg-15-manual-log\\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "7iT75t1yiSzG",
    "outputId": "bb348141-4bb1-491c-fbf4-f633ff4a2dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 19:23:23.546867: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Downloading: 100% 792k/792k [00:00<00:00, 3.31MB/s]\n",
      "Downloading: 100% 31.0/31.0 [00:00<00:00, 22.4kB/s]\n",
      "Downloading: 100% 65.0/65.0 [00:00<00:00, 41.6kB/s]\n",
      "Downloading: 100% 90.0/90.0 [00:00<00:00, 60.6kB/s]\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [01:14<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "#using originally pretrained model small t5 valhalla\n",
    "!python eval.py \\\n",
    "    --model_name_or_path valhalla/t5-small-qg-hl \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-original-small-qg-hl_tweet_manual_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "r8cysPM6hrI6",
    "outputId": "bbe0e0ce-a89a-4dee-9c58-8902d7bf14d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.168221\n",
      "Bleu_2: 0.101524\n",
      "Bleu_3: 0.067962\n",
      "Bleu_4: 0.047070\n",
      "METEOR: 0.200778\n",
      "ROUGE_L: 0.183403\n",
      "CIDEr: 0.552163\n"
     ]
    }
   ],
   "source": [
    ", #evaluate valhalla small model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-original-small-qg-hl_tweet_manual_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "YAxIlP2JTYSp",
    "outputId": "257a8f7a-30eb-48db-c272-e5884e78722a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 19:25:48.493103: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [01:08<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "#using finetuned small model with 15 epochs\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  t5-small-qg-hl-15-manual \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-small-fine-tuned-qg-hl_t5_tweet_manual.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "5wL4qdMEjNpA",
    "outputId": "43611e3f-9696-4562-ca5b-a7b0ae893247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.351202\n",
      "Bleu_2: 0.246024\n",
      "Bleu_3: 0.182722\n",
      "Bleu_4: 0.141755\n",
      "METEOR: 0.225531\n",
      "ROUGE_L: 0.371697\n",
      "CIDEr: 1.479602\n"
     ]
    }
   ],
   "source": [
    ", #evaluate finetuned small model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-small-fine-tuned-qg-hl_t5_tweet_manual.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vb5o1QGjRCbO"
   },
   "outputs": [],
   "source": [
    "#valhalla/t5-base-qg-hl\n",
    "###########################\n",
    "#08/15/2020 15:57:21 - INFO - transformers.configuration_utils -   Configuration saved in t5-small-qg-hl/config.json\n",
    "#08/15/2020 15:57:23 - INFO - transformers.modeling_utils -   Model weights saved in t5-small-qg-hl/pytorch_model.bin\n",
    "%%capture loggingfine\n",
    "!python3 run_qg.py \\\n",
    "    --model_name_or_path valhalla/t5-base-qg-hl \\\n",
    "    --model_type t5 \\\n",
    "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
    "    --output_dir t5-base-qg-hl-15-manual \\\n",
    "    --train_file_path data/train_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --seed 42 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100\\\n",
    "    --logging_dir t5-base-qg-15-manual-log\\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68900,
     "status": "ok",
     "timestamp": 1598087390746,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "qXl463kSiUd1",
    "outputId": "81936e13-3a5b-427a-e60b-80c95d6e46b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-22 09:08:49.640018: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Downloading: 100% 1.02k/1.02k [00:00<00:00, 905kB/s]\n",
      "Downloading: 100% 792k/792k [00:00<00:00, 2.42MB/s]\n",
      "Downloading: 100% 15.0/15.0 [00:00<00:00, 11.3kB/s]\n",
      "Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.41MB/s]\n",
      "Downloading: 100% 129/129 [00:00<00:00, 95.3kB/s]\n",
      "Downloading: 100% 892M/892M [00:29<00:00, 29.8MB/s]\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [78,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "  0% 0/34 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"eval.py\", line 92, in <module>\n",
      "    main()\n",
      "  File \"eval.py\", line 82, in main\n",
      "    max_length=args.max_decoding_length\n",
      "  File \"eval.py\", line 52, in get_predictions\n",
      "    length_penalty=length_penalty,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\", line 1159, in generate\n",
      "    encoder_outputs: tuple = encoder(input_ids, attention_mask=attention_mask)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\", line 748, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\", line 516, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\", line 423, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\", line 338, in forward\n",
      "    q = shape(self.q(input))  # (bs, n_heads, qlen, dim_per_head)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\", line 91, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1676, in linear\n",
      "    output = input.matmul(weight.t())\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\n"
     ]
    }
   ],
   "source": [
    "#using originally pretrained model base t5 valhalla\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  valhalla/t5-base-qg-hl \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_original_base_base-qg-hl_tweet_manual_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3529,
     "status": "ok",
     "timestamp": 1598051854833,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "ruGrhXa-jEs4",
    "outputId": "7898710d-997e-4d45-c22a-a6315cffd15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: nlg-eval [OPTIONS]\n",
      "Try 'nlg-eval --help' for help.\n",
      "\n",
      "Error: Invalid value for '--hypothesis': Path 'hypothesis_original_base_base-qg-hl_tweet_manual_model.txt' does not exist.\n"
     ]
    }
   ],
   "source": [
    ", #evaluate valhalla base model\n",
    "!nlg-eval --hypothesis=hypothesis_original_base_base-qg-hl_tweet_manual_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2729,
     "status": "ok",
     "timestamp": 1598093731392,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "JZ_Le-kEUKHM",
    "outputId": "b8e593d8-7bfa-4e14-9a08-3ba961712dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'eval.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#using originally pretrained model base t5 valhalla\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  t5-base-qg-hl-15-manual \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_fine-tuned_base_qg-hl_tweet_manual_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17288,
     "status": "ok",
     "timestamp": 1598052027059,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "jp6-PavPUZf5",
    "outputId": "21a2624e-0985-4ea3-bebc-4a446fd8bb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.400382\n",
      "Bleu_2: 0.293443\n",
      "Bleu_3: 0.222170\n",
      "Bleu_4: 0.173488\n",
      "METEOR: 0.252304\n",
      "ROUGE_L: 0.423129\n",
      "CIDEr: 1.833456\n"
     ]
    }
   ],
   "source": [
    "#evaluate fine-tuned base model\n",
    "!nlg-eval --hypothesis=hypothesis_fine-tuned_base_qg-hl_tweet_manual_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rFUnxlxwe3ub"
   },
   "outputs": [],
   "source": [
    "#valhalla/t5-small-e2e-qg\n",
    "###########################\n",
    "#08/15/2020 15:57:21 - INFO - transformers.configuration_utils -   Configuration saved in t5-small-qg-hl/config.json\n",
    "#08/15/2020 15:57:23 - INFO - transformers.modeling_utils -   Model weights saved in t5-small-qg-hl/pytorch_model.bin\n",
    "%%capture loggingfine\n",
    "!python3 run_qg.py \\\n",
    "    --model_name_or_path valhalla/t5-small-e2e-qg \\\n",
    "    --model_type t5 \\\n",
    "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
    "    --output_dir t5-small-E2E-qg-15-manual \\\n",
    "    --train_file_path data/train_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --seed 42 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100\\\n",
    "    --logging_dir t5-small-E2E-qg-15-manual-log\\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 72642,
     "status": "ok",
     "timestamp": 1598053602561,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "tJCH9BqVjW3Y",
    "outputId": "01e961e7-f752-447c-84db-8fca63122872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 23:45:32.858092: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Downloading: 100% 792k/792k [00:00<00:00, 1.84MB/s]\n",
      "Downloading: 100% 31.0/31.0 [00:00<00:00, 36.6kB/s]\n",
      "Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.80MB/s]\n",
      "Downloading: 100% 124/124 [00:00<00:00, 134kB/s]\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [00:49<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "#using ORIGINAL small valhalla/t5-small-e2e-qg 15 epochs\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  valhalla/t5-small-e2e-qg \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-small-original-E2E-qg-hl_t5_tweet_manual.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16760,
     "status": "ok",
     "timestamp": 1598053621526,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "EBiNTvzqjZWb",
    "outputId": "5923259d-f412-4da5-a98b-ddc73287dffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.053089\n",
      "Bleu_2: 0.026385\n",
      "Bleu_3: 0.015345\n",
      "Bleu_4: 0.009345\n",
      "METEOR: 0.109112\n",
      "ROUGE_L: 0.082893\n",
      "CIDEr: 0.025654\n"
     ]
    }
   ],
   "source": [
    "#evaluate ORGIINAL E2E small model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-small-original-E2E-qg-hl_t5_tweet_manual.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46098,
     "status": "ok",
     "timestamp": 1598053667559,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "3MFGnqYcjwt6",
    "outputId": "53b04222-8616-4158-b567-c7233200c572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 23:47:03.634999: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [00:34<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "#using FINE-TUNED small valhalla/t5-small-e2e-qg 15 epochs\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  t5-small-E2E-qg-15-manual \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-small-fine-tuned-E2E-qg-hl_t5_tweet_manual.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15393,
     "status": "ok",
     "timestamp": 1598053691864,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "nA0oGFJfkD1D",
    "outputId": "d0f8f89e-fcf0-45c6-9b1e-8862828ab384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.333872\n",
      "Bleu_2: 0.230703\n",
      "Bleu_3: 0.169098\n",
      "Bleu_4: 0.129525\n",
      "METEOR: 0.215522\n",
      "ROUGE_L: 0.353338\n",
      "CIDEr: 1.348371\n"
     ]
    }
   ],
   "source": [
    "#evaluate fine-tuned small model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-small-fine-tuned-E2E-qg-hl_t5_tweet_manual.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sd-hW9DZe4fl"
   },
   "outputs": [],
   "source": [
    "#valhalla/t5-base-e2e-qg\n",
    "##########################\n",
    "#08/15/2020 15:57:21 - INFO - transformers.configuration_utils -   Configuration saved in t5-small-qg-hl/config.json\n",
    "#08/15/2020 15:57:23 - INFO - transformers.modeling_utils -   Model weights saved in t5-small-qg-hl/pytorch_model.bin\n",
    "%%capture loggingfine\n",
    "!python3 run_qg.py \\\n",
    "    --model_name_or_path valhalla/t5-base-e2e-qg \\\n",
    "    --model_type t5 \\\n",
    "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
    "    --output_dir t5-base-E2E-qg-15-manual \\\n",
    "    --train_file_path data/train_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --seed 42 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100\\\n",
    "    --logging_dir t5-base-E2E-qg-15-manual-log\\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 120166,
     "status": "ok",
     "timestamp": 1598087754143,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "_OXfJYDNjnLe",
    "outputId": "5518ba95-4c3e-4269-d916-ebe6ff49ddba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-22 09:13:56.967399: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Downloading: 100% 1.35k/1.35k [00:00<00:00, 1.02MB/s]\n",
      "Downloading: 100% 792k/792k [00:00<00:00, 2.32MB/s]\n",
      "Downloading: 100% 31.0/31.0 [00:00<00:00, 25.9kB/s]\n",
      "Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.58MB/s]\n",
      "Downloading: 100% 195/195 [00:00<00:00, 168kB/s]\n",
      "Downloading: 100% 892M/892M [00:27<00:00, 32.4MB/s]\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [01:10<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "#using ORIGINAL base valhalla/t5-base-e2e-qg 15 epochs\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  valhalla/t5-base-e2e-qg \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-base-original-E2E-qg-hl_t5_tweet_manual.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17048,
     "status": "ok",
     "timestamp": 1598087778959,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "uLtr8dY8jrsk",
    "outputId": "2cba3cf3-17fd-4c0a-e35f-2acc6f53d81c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.065108\n",
      "Bleu_2: 0.035949\n",
      "Bleu_3: 0.022479\n",
      "Bleu_4: 0.014611\n",
      "METEOR: 0.127798\n",
      "ROUGE_L: 0.101185\n",
      "CIDEr: 0.048939\n"
     ]
    }
   ],
   "source": [
    "#evaluate ORIGINAL E2E base model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-base-original-E2E-qg-hl_t5_tweet_manual.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 82192,
     "status": "ok",
     "timestamp": 1598087866069,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "hybF1Zzej9Gm",
    "outputId": "b81a59a0-a89f-44c2-854a-a9589f03b3de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-22 09:16:26.929314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [00:51<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "#using FINE-TUNED base valhalla/t5-base-e2e-qg 15 epochs\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  t5-base-E2E-qg-15-manual \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_manual.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-base-fine-tuned-E2E-qg-hl_t5_tweet_manual.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14656,
     "status": "ok",
     "timestamp": 1598087888618,
     "user": {
      "displayName": "Mai6ki Mai6ka",
      "photoUrl": "",
      "userId": "06470849772540350594"
     },
     "user_tz": 240
    },
    "id": "qedeX-kVkPw-",
    "outputId": "8de89086-eab7-475a-b785-99b12a4909df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.399541\n",
      "Bleu_2: 0.291154\n",
      "Bleu_3: 0.219786\n",
      "Bleu_4: 0.170864\n",
      "METEOR: 0.251499\n",
      "ROUGE_L: 0.422600\n",
      "CIDEr: 1.826197\n"
     ]
    }
   ],
   "source": [
    "#evaluate fine-tuned base model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-base-fine-tuned-E2E-qg-hl_t5_tweet_manual.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tweet_question_generation_corrected_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
