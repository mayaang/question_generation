{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patil-suraj/question_generation/blob/master/question_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBKGoeg9KM5k"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "colab_type": "code",
    "id": "aweZgxXBDsOQ",
    "outputId": "da36be6b-c055-4a4b-d513-c5d7d4858dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
      "\r",
      "\u001b[K     |▍                               | 10kB 25.1MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 20kB 6.1MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 30kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 40kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 51kB 7.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 61kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 71kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 81kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 92kB 7.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 102kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 112kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 122kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 133kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 143kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 153kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 163kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 174kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 184kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 194kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 204kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 215kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 225kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 235kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 245kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 256kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 266kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 276kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 286kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 296kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 307kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 317kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 327kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 337kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 348kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 358kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 368kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 378kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 389kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 399kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 409kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 419kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 430kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 440kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 450kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 460kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 471kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 481kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 491kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 501kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 512kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 522kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 532kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 542kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 552kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 563kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 573kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 583kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 593kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 604kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 614kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 624kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 634kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 645kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 655kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 665kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 675kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 686kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 696kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 706kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 716kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 727kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 737kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 747kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 757kB 8.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.7)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 30.1MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.8.0-rc4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 53.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 51.2MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (0.16.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=18da418add10612ee7e4dce13f7d664e5ce7158cd28f67c56350ba03e3e33024\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "4FARi6xuQ4IZ",
    "outputId": "ce7d7dfc-6698-4c7c-e1ca-c1a98f529ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "colab_type": "code",
    "id": "iMjtG0q-Ws9B",
    "outputId": "347d3897-1224-4351-bb25-9dcfb15a8242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 5.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp) (1.0.5)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 31.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
      "Collecting pyarrow>=0.16.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/99/0a605f016121ca314d1469dc9069e4978395bc46fda40f73099d90ad3ba4/pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3MB 172kB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->nlp) (1.15.0)\n",
      "Installing collected packages: xxhash, pyarrow, nlp\n",
      "  Found existing installation: pyarrow 0.14.1\n",
      "    Uninstalling pyarrow-0.14.1:\n",
      "      Successfully uninstalled pyarrow-0.14.1\n",
      "Successfully installed nlp-0.4.0 pyarrow-1.0.1 xxhash-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "colab_type": "code",
    "id": "kvV3hytU59ab",
    "outputId": "862e099d-d3f5-4fe2-90e6-4d87536e8289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Maluuba/nlg-eval.git@master\n",
      "  Cloning https://github.com/Maluuba/nlg-eval.git (to revision master) to /tmp/pip-req-build-o2oczgtz\n",
      "  Running command git clone -q https://github.com/Maluuba/nlg-eval.git /tmp/pip-req-build-o2oczgtz\n",
      "Requirement already satisfied: click>=6.3 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (7.1.2)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (3.2.5)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.18.5)\n",
      "Collecting psutil>=5.6.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/3e/d18f2c04cf2b528e18515999b0c8e698c136db78f62df34eee89cee205f1/psutil-5.7.2.tar.gz (460kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 8.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (2.23.0)\n",
      "Requirement already satisfied: six>=1.11 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (0.22.2.post1)\n",
      "Requirement already satisfied: gensim>=3 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (3.6.0)\n",
      "Requirement already satisfied: Theano>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.0.5)\n",
      "Requirement already satisfied: tqdm>=4.24 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (4.41.1)\n",
      "Collecting xdg\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/7b/6ad85311fd715df37ef9bb17ad1b26e26b4cdd69c7e1e7e285422b83a7e1/xdg-4.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (3.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17->nlg-eval==2.3) (0.16.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3->nlg-eval==2.3) (2.1.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (1.14.37)\n",
      "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (2.49.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (1.17.37)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.2.1->gensim>=3->nlg-eval==2.3) (0.15.2)\n",
      "Building wheels for collected packages: nlg-eval, psutil\n",
      "  Building wheel for nlg-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nlg-eval: filename=nlg_eval-2.3-cp36-none-any.whl size=68175138 sha256=b49679d19a6c4bc9c59fa529df20e883bda901fbf5aff40af454d12daec933e0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9jw4dd6q/wheels/a5/7c/fd/f312beca2adcc3f49cb40570730658dad37bb5709f5d237a56\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for psutil: filename=psutil-5.7.2-cp36-cp36m-linux_x86_64.whl size=279890 sha256=cccfb1921f894ff9c4def656cfe7848d2e4a3a45808f01860edf63fc57e1cb9e\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/a0/f5/c4fa280463e29aea07797acb5312358fefb067c1f4f98e11b1\n",
      "Successfully built nlg-eval psutil\n",
      "Installing collected packages: psutil, xdg, nlg-eval\n",
      "  Found existing installation: psutil 5.4.8\n",
      "    Uninstalling psutil-5.4.8:\n",
      "      Successfully uninstalled psutil-5.4.8\n",
      "Successfully installed nlg-eval-2.3 psutil-5.7.2 xdg-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install git+https://github.com/Maluuba/nlg-eval.git@master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "MRHG89D_6RPm",
    "outputId": "3d93ae59-7bfd-40a5-c629-84c3aff5bfce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\u001b[31mInstalling to /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[31mIn case of incomplete downloads, delete the directory and run `nlg-eval --setup /root/.cache/nlgeval' again.\u001b[0m\n",
      "Downloading https://raw.githubusercontent.com/robmsmt/glove-gensim/4c2224bccd61627b76c50a5e1d6afd1c82699d22/glove2word2vec.py to /usr/local/lib/python3.6/dist-packages/nlgeval/word2vec.\n",
      "Downloading http://nlp.stanford.edu/data/glove.6B.zip to /root/.cache/nlgeval.\n",
      "Downloading http://www.cs.toronto.edu/~rkiros/models/utable.npy to /root/.cache/nlgeval.\n",
      "Downloading http://www.cs.toronto.edu/~rkiros/models/dictionary.txt to /root/.cache/nlgeval.\n",
      "glove2word2vec.py: 100% 1.00/1.00 [00:00<00:00, 515 chunks/s]\n",
      "Downloading http://www.cs.toronto.edu/~rkiros/models/btable.npy to /root/.cache/nlgeval.\n",
      "dictionary.txt: 550 chunks [00:00, 656 chunks/s]\n",
      "Downloading http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz to /root/.cache/nlgeval.\n",
      "uni_skip.npz: 100% 634/634 [00:21<00:00, 29.3 chunks/s]\n",
      "Downloading http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl to /root/.cache/nlgeval.\n",
      "uni_skip.npz.pkl: 100% 1.00/1.00 [00:00<00:00, 1.52k chunks/s]\n",
      "Downloading http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz to /root/.cache/nlgeval.\n",
      "bi_skip.npz: 100% 276/276 [00:24<00:00, 11.1 chunks/s]\n",
      "Downloading http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl to /root/.cache/nlgeval.\n",
      "bi_skip.npz.pkl: 100% 1.00/1.00 [00:00<00:00, 1.93k chunks/s]\n",
      "Downloading https://raw.githubusercontent.com/moses-smt/mosesdecoder/b199e654df2a26ea58f234cbb642e89d9c1f269d/scripts/generic/multi-bleu.perl to /usr/local/lib/python3.6/dist-packages/nlgeval/multibleu.\n",
      "multi-bleu.perl: 100% 1.00/1.00 [00:00<00:00, 497 chunks/s]\n",
      "utable.npy: 100% 2.23k/2.23k [01:54<00:00, 19.5 chunks/s]\n",
      "btable.npy: 100% 2.23k/2.23k [01:59<00:00, 18.7 chunks/s]\n",
      "glove.6B.zip: 100% 823/823 [06:29<00:00, 2.11 chunks/s]\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2020-08-21 17:29:05,800 : MainThread : INFO : 400000 lines with 300 dimensions\n",
      "2020-08-21 17:29:18,084 : MainThread : INFO : Model /root/.cache/nlgeval/glove.6B.300d.model.txt successfully created !!\n",
      "2020-08-21 17:29:18,084 : MainThread : INFO : loading projection weights from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
      "2020-08-21 17:31:06,439 : MainThread : INFO : loaded (400000, 300) matrix from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
      "2020-08-21 17:31:06,440 : MainThread : INFO : precomputing L2-norms of word weight vectors\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2020-08-21 17:31:07,245 : MainThread : INFO : Most similar to king are: [('queen', 0.6336469054222107), ('prince', 0.619662344455719), ('monarch', 0.5899620652198792), ('kingdom', 0.5791267156600952), ('throne', 0.5606487989425659), ('ii', 0.5562329888343811), ('iii', 0.5503199100494385), ('crown', 0.5224862694740295), ('reign', 0.521735429763794), ('kings', 0.5066401362419128)]\n",
      "2020-08-21 17:31:07,245 : MainThread : INFO : Similarity score between woman and man is 0.6998663 \n",
      "2020-08-21 17:31:07,245 : MainThread : INFO : Finished running --setup\n",
      "2020-08-21 17:31:07,348 : MainThread : INFO : loading projection weights from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
      "2020-08-21 17:32:51,110 : MainThread : INFO : loaded (400000, 300) matrix from /root/.cache/nlgeval/glove.6B.300d.model.txt\n",
      "2020-08-21 17:32:51,111 : MainThread : INFO : saving Word2VecKeyedVectors object under /root/.cache/nlgeval/glove.6B.300d.model.bin, separately None\n",
      "2020-08-21 17:32:51,111 : MainThread : INFO : storing np array 'vectors' to /root/.cache/nlgeval/glove.6B.300d.model.bin.vectors.npy\n",
      "2020-08-21 17:32:52,265 : MainThread : INFO : not storing attribute vectors_norm\n",
      "2020-08-21 17:32:53,060 : MainThread : INFO : saved /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
      "2020-08-21 17:32:53,060 : MainThread : INFO : loading Word2VecKeyedVectors object from /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
      "2020-08-21 17:32:53,922 : MainThread : INFO : loading vectors from /root/.cache/nlgeval/glove.6B.300d.model.bin.vectors.npy with mmap=r\n",
      "2020-08-21 17:32:53,924 : MainThread : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-08-21 17:32:53,925 : MainThread : INFO : loaded /root/.cache/nlgeval/glove.6B.300d.model.bin\n",
      "WARNING: could not read rc.json in /root/.config/nlgeval, overwriting\n"
     ]
    }
   ],
   "source": [
    "!nlg-eval --setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "-vXCbg-5ImHI",
    "outputId": "06fdd813-7434-4c12-f916-b035615aba5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oy_T1CiVVNuH",
    "outputId": "e350f5f1-0b40-4346-8416-423affa230d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/question_generation/question_generation\n"
     ]
    }
   ],
   "source": [
    "%cd drive/My\\ Drive/question_generation/question_generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "iUcNtTKyIjK3",
    "outputId": "9b186df4-ad70-42d5-84b4-ada6ed83e760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/\n",
      "data_collator.py\n",
      "eval.py\n",
      "hypothesis_t5-base-fine-tuned-qg-hl_t5_tweet_manual.txt\n",
      "hypothesis_t5-small-fine-tuned-qg-hl_t5_tweet_manual.txt\n",
      "hypothesis_t5-valhalla-small-qg-hl_tweet_manual_model.txt\n",
      "hypothesis_valhalla_base_base-qg-hl_tweet_manual_model.txt\n",
      "LICENSE\n",
      "\u001b[01;34mlyrics\u001b[0m/\n",
      "\u001b[01;34mnotebooks\u001b[0m/\n",
      "pipelines.py\n",
      "prepare_data.py\n",
      "question_generation.ipynb\n",
      "README.md\n",
      "run_qg.py\n",
      "\u001b[01;34mt5-base-qg-hl-15-automatic\u001b[0m/\n",
      "\u001b[01;34mt5_qg_tokenizer\u001b[0m/\n",
      "\u001b[01;34mt5-small-qg-hl-15-automatic\u001b[0m/\n",
      "trainer.py\n",
      "\u001b[01;34mtweetqadata\u001b[0m/\n",
      "utils.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6Okz9rhOWhiP",
    "outputId": "7973a2d1-56a4-4b2f-bf20-4b2ee372f06d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 00:00:47.714271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "08/21/2020 00:00:50 - INFO - filelock -   Lock 139917298027880 acquired on /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f.lock\n",
      "08/21/2020 00:00:50 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpum9svjaw\n",
      "Downloading: 100% 792k/792k [00:00<00:00, 2.32MB/s]\n",
      "08/21/2020 00:00:50 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model in cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
      "08/21/2020 00:00:50 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
      "08/21/2020 00:00:50 - INFO - filelock -   Lock 139917298027880 released on /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f.lock\n",
      "08/21/2020 00:00:50 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n",
      "08/21/2020 00:00:50 - INFO - transformers.tokenization_utils -   Adding <sep> to the vocabulary\n",
      "08/21/2020 00:00:50 - INFO - transformers.tokenization_utils -   Adding <hl> to the vocabulary\n",
      "08/21/2020 00:00:52 - INFO - nlp.load -   Checking data/tweet_multitask/tweet_multitask.py for additional imports.\n",
      "08/21/2020 00:00:52 - INFO - filelock -   Lock 139916123831880 acquired on data/tweet_multitask/tweet_multitask.py.lock\n",
      "08/21/2020 00:00:52 - INFO - nlp.load -   Creating main folder for dataset data/tweet_multitask/tweet_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_multitask\n",
      "08/21/2020 00:00:52 - INFO - nlp.load -   Creating specific version folder for dataset data/tweet_multitask/tweet_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_multitask/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530\n",
      "08/21/2020 00:00:52 - INFO - nlp.load -   Copying script file from data/tweet_multitask/tweet_multitask.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_multitask/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tweet_multitask.py\n",
      "08/21/2020 00:00:52 - INFO - nlp.load -   Couldn't find dataset infos file at data/tweet_multitask/dataset_infos.json\n",
      "08/21/2020 00:00:52 - INFO - nlp.load -   Creating metadata file for dataset data/tweet_multitask/tweet_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_multitask/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tweet_multitask.json\n",
      "08/21/2020 00:00:52 - INFO - filelock -   Lock 139916123831880 released on data/tweet_multitask/tweet_multitask.py.lock\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "08/21/2020 00:00:53 - INFO - nlp.builder -   Generating dataset squad_multitask (/root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530)\n",
      "Downloading and preparing dataset squad_multitask/highlight_qg_format (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530...\n",
      "08/21/2020 00:00:53 - INFO - nlp.builder -   Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "08/21/2020 00:00:53 - INFO - nlp.utils.info_utils -   Unable to verify checksums.\n",
      "08/21/2020 00:00:53 - INFO - nlp.builder -   Generating split train\n",
      "0 examples [00:00, ? examples/s]08/21/2020 00:00:53 - INFO - root -   generating examples from = /content/drive/My Drive/question_generation/question_generation/data/tweet_multitask/tweet_train_squad_automatic.json\n",
      "08/21/2020 00:00:55 - INFO - nlp.arrow_writer -   Done writing 41362 examples in 10283799 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530.incomplete/squad_multitask-train.arrow.\n",
      "08/21/2020 00:00:55 - INFO - nlp.builder -   Generating split validation\n",
      "0 examples [00:00, ? examples/s]08/21/2020 00:00:55 - INFO - root -   generating examples from = /content/drive/My Drive/question_generation/question_generation/data/tweet_multitask/tweet_dev_squad_automatic.json\n",
      "08/21/2020 00:00:56 - INFO - nlp.arrow_writer -   Done writing 3454 examples in 882245 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530.incomplete/squad_multitask-validation.arrow.\n",
      "08/21/2020 00:00:56 - INFO - nlp.utils.info_utils -   Unable to verify splits sizes.\n",
      "Dataset squad_multitask downloaded and prepared to /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530. Subsequent calls will reuse this data.\n",
      "08/21/2020 00:00:56 - INFO - nlp.builder -   Constructing Dataset for split train, from /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530\n",
      "08/21/2020 00:00:56 - INFO - nlp.utils.info_utils -   Unable to verify checksums.\n",
      "08/21/2020 00:00:56 - INFO - nlp.load -   Checking data/tweet_multitask/tweet_multitask.py for additional imports.\n",
      "08/21/2020 00:00:56 - INFO - filelock -   Lock 139916450109760 acquired on data/tweet_multitask/tweet_multitask.py.lock\n",
      "08/21/2020 00:00:56 - INFO - nlp.load -   Found main folder for dataset data/tweet_multitask/tweet_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_multitask\n",
      "08/21/2020 00:00:56 - INFO - nlp.load -   Found specific version folder for dataset data/tweet_multitask/tweet_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_multitask/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530\n",
      "08/21/2020 00:00:56 - INFO - nlp.load -   Found script file from data/tweet_multitask/tweet_multitask.py to /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_multitask/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tweet_multitask.py\n",
      "08/21/2020 00:00:56 - INFO - nlp.load -   Couldn't find dataset infos file at data/tweet_multitask/dataset_infos.json\n",
      "08/21/2020 00:00:56 - INFO - nlp.load -   Found metadata file for dataset data/tweet_multitask/tweet_multitask.py at /usr/local/lib/python3.6/dist-packages/nlp/datasets/tweet_multitask/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tweet_multitask.json\n",
      "08/21/2020 00:00:56 - INFO - filelock -   Lock 139916450109760 released on data/tweet_multitask/tweet_multitask.py.lock\n",
      "08/21/2020 00:00:56 - INFO - nlp.builder -   Overwrite dataset info from restored data version.\n",
      "08/21/2020 00:00:56 - INFO - nlp.info -   Loading Dataset info from /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530\n",
      "08/21/2020 00:00:56 - INFO - nlp.builder -   Reusing dataset squad_multitask (/root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530)\n",
      "08/21/2020 00:00:56 - INFO - nlp.builder -   Constructing Dataset for split validation, from /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530\n",
      "08/21/2020 00:00:56 - INFO - nlp.utils.info_utils -   Unable to verify checksums.\n",
      "08/21/2020 00:00:56 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/cache-71d2e6d66a3139986539d64a18e8dab6.arrow\n",
      "100% 42/42 [00:00<00:00, 160.52it/s]\n",
      "08/21/2020 00:00:56 - INFO - nlp.arrow_writer -   Done writing 10691 examples in 2872131 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tmppfn78i2q.\n",
      "08/21/2020 00:00:56 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/cache-72996934bf2eeece8fe3ed99e9b476b4.arrow\n",
      "100% 4/4 [00:00<00:00, 173.45it/s]\n",
      "08/21/2020 00:00:56 - INFO - nlp.arrow_writer -   Done writing 1085 examples in 283614 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tmp095qzh49.\n",
      "08/21/2020 00:00:56 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/cache-68fc1b3405ac0838019b500fe74b7ba2.arrow\n",
      "100% 10691/10691 [00:00<00:00, 32402.71it/s]\n",
      "08/21/2020 00:00:57 - INFO - nlp.arrow_writer -   Done writing 10691 examples in 2978669 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tmp01qugb4u.\n",
      "08/21/2020 00:00:57 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/cache-c67a519c6ba434a181ffc564b2f9b41f.arrow\n",
      "100% 10691/10691 [00:00<00:00, 33349.38it/s]\n",
      "08/21/2020 00:00:57 - INFO - nlp.arrow_writer -   Done writing 10691 examples in 2850377 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tmpqwbb4qtz.\n",
      "08/21/2020 00:00:57 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/cache-5ba125b72505e29664d7380f3caf61dd.arrow\n",
      "100% 11/11 [00:23<00:00,  2.15s/it]\n",
      "08/21/2020 00:01:21 - INFO - nlp.arrow_writer -   Done writing 10691 examples in 93296369 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tmpt3y84ddi.\n",
      "08/21/2020 00:01:21 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/cache-003903e60806388b561ea95d68933fbd.arrow\n",
      "100% 1085/1085 [00:00<00:00, 33181.33it/s]\n",
      "08/21/2020 00:01:21 - INFO - nlp.arrow_writer -   Done writing 1085 examples in 294440 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tmp4sul5xi6.\n",
      "08/21/2020 00:01:21 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/cache-a4905351ca814a84e012619f6bb7e18e.arrow\n",
      "100% 1085/1085 [00:00<00:00, 33228.82it/s]\n",
      "08/21/2020 00:01:21 - INFO - nlp.arrow_writer -   Done writing 1085 examples in 281420 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tmpd3vnajlx.\n",
      "08/21/2020 00:01:21 - INFO - nlp.arrow_dataset -   Caching processed dataset at /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/cache-3cf1d64ea276484db1fcf82454cec5dc.arrow\n",
      "100% 2/2 [00:02<00:00,  1.25s/it]\n",
      "08/21/2020 00:01:23 - INFO - nlp.arrow_writer -   Done writing 1085 examples in 9460544 bytes /root/.cache/huggingface/datasets/squad_multitask/highlight_qg_format/1.0.0/52dcd6c1adc13b73bc6f2b3aab97b2df72b7363b1265ed27c133ca9d60733530/tmppsh16jhe.\n",
      "08/21/2020 00:01:23 - INFO - nlp.arrow_dataset -   Set __getitem__(key) output type to torch for ['source_ids', 'target_ids', 'attention_mask'] columns  (when key is int or slice) and don't output other (un-formated) columns.\n",
      "08/21/2020 00:01:23 - INFO - nlp.arrow_dataset -   Set __getitem__(key) output type to torch for ['source_ids', 'target_ids', 'attention_mask'] columns  (when key is int or slice) and don't output other (un-formated) columns.\n",
      "08/21/2020 00:01:25 - INFO - __main__ -   saved train dataset at data/train_data_qg_hl_t5_tweet_automatic.pt\n",
      "08/21/2020 00:01:26 - INFO - __main__ -   saved validation dataset at data/valid_data_qg_hl_t5_tweet_automatic.pt\n",
      "08/21/2020 00:01:27 - INFO - __main__ -   saved tokenizer at t5_qg_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Already DONE !!!!!\n",
    "#!python3 prepare_data.py --task qg --model_type t5 --dataset_path data/tweet_multitask --qg_format highlight_qg_format --max_source_length 512 --max_target_length 32 --train_file_name train_data_qg_hl_t5_tweet_automatic.pt  --valid_file_name valid_data_qg_hl_t5_tweet_automatic.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yoRJR-uEg4Wu"
   },
   "outputs": [],
   "source": [
    "#Saving model checkpoint to t5-small-qg-hl-15\n",
    "#08/15/2020 15:57:21 - INFO - transformers.configuration_utils -   Configuration saved in t5-small-qg-hl/config.json\n",
    "#08/15/2020 15:57:23 - INFO - transformers.modeling_utils -   Model weights saved in t5-small-qg-hl/pytorch_model.bin\n",
    "###############################\n",
    "# valhalla/t5-small-qg-hl\n",
    "# DONE\n",
    "###############################\n",
    "%%capture loggingfine\n",
    "!python3 run_qg.py \\\n",
    "    --model_name_or_path valhalla/t5-small-qg-hl \\\n",
    "    --model_type t5 \\\n",
    "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
    "    --output_dir t5-small-qg-hl-15-automatic \\\n",
    "    --train_file_path data/train_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --seed 42 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100\\\n",
    "    --logging_dir t5-small-qg-hl-15-automatic-log\\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "7iT75t1yiSzG",
    "outputId": "54f53551-23fe-446a-b657-612e5d2311f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 13:12:48.915849: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Downloading: 100% 627/627 [00:00<00:00, 425kB/s]\n",
      "Downloading: 100% 792k/792k [00:01<00:00, 723kB/s]\n",
      "Downloading: 100% 31.0/31.0 [00:00<00:00, 16.6kB/s]\n",
      "Downloading: 100% 65.0/65.0 [00:00<00:00, 38.3kB/s]\n",
      "Downloading: 100% 90.0/90.0 [00:00<00:00, 54.0kB/s]\n",
      "Downloading: 100% 242M/242M [00:04<00:00, 57.6MB/s]\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [01:12<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# DONE\n",
    "# using ORIGINAL pretrained small model valhalla\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  valhalla/t5-small-qg-hl \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-small-qg-hl_tweet_atomatic_original_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "zChPaou5BhFg",
    "outputId": "a8a025fd-b9a6-4f4b-98cb-f4b8ccb8c791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.165630\n",
      "Bleu_2: 0.099125\n",
      "Bleu_3: 0.065634\n",
      "Bleu_4: 0.045238\n",
      "METEOR: 0.198628\n",
      "ROUGE_L: 0.179915\n",
      "CIDEr: 0.532227\n"
     ]
    }
   ],
   "source": [
    "#evaluate valhalla model NOW\n",
    "!nlg-eval --hypothesis=hypothesis_t5-small-qg-hl_tweet_atomatic_original_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "n0VfwGq0f-7G",
    "outputId": "cbd62545-3427-4479-cc5c-f4b79452e698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 14:19:52.023581: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [01:06<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "#using FINE TUNED small model valhalla\n",
    "#DONE !!!\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  t5-small-qg-hl-15-automatic \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-small-qg-hl_tweet_atomatic_fine-tuned_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "QRnGl6_SH6q9",
    "outputId": "b189147a-701c-4085-a2a1-d8313fa2d564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.346000\n",
      "Bleu_2: 0.241399\n",
      "Bleu_3: 0.178444\n",
      "Bleu_4: 0.138167\n",
      "METEOR: 0.222483\n",
      "ROUGE_L: 0.367536\n",
      "CIDEr: 1.431858\n"
     ]
    }
   ],
   "source": [
    "#evaluate valhalla model finetuned small\n",
    "!nlg-eval --hypothesis=hypothesis_t5-small-qg-hl_tweet_atomatic_fine-tuned_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ObsXDiNhM-M"
   },
   "outputs": [],
   "source": [
    "#DONE\n",
    "###############################\n",
    "# valhalla/t5-base-qg-hl\n",
    "###############################\n",
    "%%capture loggingfine\n",
    "!python3 run_qg.py \\\n",
    "    --model_name_or_path valhalla/t5-base-qg-hl \\\n",
    "    --model_type t5 \\\n",
    "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
    "    --output_dir t5-base-qg-hl-15-automatic \\\n",
    "    --train_file_path data/train_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --seed 42 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100 \\\n",
    "    --logging_dir t5-base-qg-hl-15-automatic-log\\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YAxIlP2JTYSp",
    "outputId": "129fd59c-699a-4110-f08c-902ff463460d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 18:04:23.726635: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [221,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "  0% 0/34 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"eval.py\", line 92, in <module>\n",
      "    main()\n",
      "  File \"eval.py\", line 82, in main\n",
      "    max_length=args.max_decoding_length\n",
      "  File \"eval.py\", line 52, in get_predictions\n",
      "    length_penalty=length_penalty,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\", line 1159, in generate\n",
      "    encoder_outputs: tuple = encoder(input_ids, attention_mask=attention_mask)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\", line 748, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\", line 516, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\", line 423, in forward\n",
      "    output_attentions=output_attentions,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\", line 338, in forward\n",
      "    q = shape(self.q(input))  # (bs, n_heads, qlen, dim_per_head)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\", line 91, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1676, in linear\n",
      "    output = input.matmul(weight.t())\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\n"
     ]
    }
   ],
   "source": [
    "# using ORIGINAL base model\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  valhalla/t5-base-qg-hl \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-base-qg-hl_tweet_atomatic_original_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "iQMbi-9NGcsq",
    "outputId": "a8a025fd-b9a6-4f4b-98cb-f4b8ccb8c791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.165630\n",
      "Bleu_2: 0.099125\n",
      "Bleu_3: 0.065634\n",
      "Bleu_4: 0.045238\n",
      "METEOR: 0.198628\n",
      "ROUGE_L: 0.179915\n",
      "CIDEr: 0.532227\n"
     ]
    }
   ],
   "source": [
    "#evaluate valhalla model NOW\n",
    "!nlg-eval --hypothesis=hypothesis_t5-small-qg-hl_tweet_atomatic_original_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "As_44vOpgUZg",
    "outputId": "4accb42a-e459-4596-e317-8a2bb5dbb0c8"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-25efa9a11ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#using FINE-TUNED base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#!pro blem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python eval.py     --model_name_or_path  t5-base-qg-hl-15-automatic     --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt     --model_type t5     --num_beams 4     --max_decoding_length 32     --output_path hypothesis_t5-base-qg-hl_tweet_atomatic_fine-tuned_model.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#using FINE-TUNED base model\n",
    "#!pro blem\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  t5-base-qg-hl-15-automatic \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-base-qg-hl_tweet_atomatic_fine-tuned_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "MOYb_qWNKKR5",
    "outputId": "4058d0dc-68ee-4f23-91ed-7aed0f460453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.394109\n",
      "Bleu_2: 0.284917\n",
      "Bleu_3: 0.214532\n",
      "Bleu_4: 0.166719\n",
      "METEOR: 0.248101\n",
      "ROUGE_L: 0.417516\n",
      "CIDEr: 1.789576\n"
     ]
    }
   ],
   "source": [
    "#evaluate valhalla model NOW\n",
    "!nlg-eval --hypothesis=hypothesis_t5-base-qg-hl_tweet_atomatic_fine-tuned_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jC9i5ZuZnnzi"
   },
   "outputs": [],
   "source": [
    "DONE\n",
    "###############################\n",
    "# valhalla/t5-small-e2e-qg\n",
    "###############################\n",
    "%%capture loggingfine\n",
    "!python3 run_qg.py \\\n",
    "    --model_name_or_path valhalla/t5-small-e2e-qg \\\n",
    "    --model_type t5 \\\n",
    "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
    "    --output_dir t5-small-E2E-qg-15-automatic \\\n",
    "    --train_file_path data/train_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --seed 42 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100 \\\n",
    "    --logging_dir t5-base-E2E-qg-15-automatic-log\\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "Zr0n8N2ig9-b",
    "outputId": "5066e156-c59d-4eca-a4d3-f828bdb11ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 13:17:15.502879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Downloading: 100% 792k/792k [00:01<00:00, 716kB/s]\n",
      "Downloading: 100% 31.0/31.0 [00:00<00:00, 19.7kB/s]\n",
      "Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.12MB/s]\n",
      "Downloading: 100% 124/124 [00:00<00:00, 75.9kB/s]\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [01:32<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "#using ORIGINAL e2e small model\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  valhalla/t5-small-e2e-qg \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-small-E2E-qg_tweet_atomatic_original_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "ZW1UQvQRKaXn",
    "outputId": "bf817ddf-7e33-40c7-eabe-19157b448693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.053403\n",
      "Bleu_2: 0.026391\n",
      "Bleu_3: 0.015307\n",
      "Bleu_4: 0.009273\n",
      "METEOR: 0.109512\n",
      "ROUGE_L: 0.083067\n",
      "CIDEr: 0.026161\n"
     ]
    }
   ],
   "source": [
    "#evaluate valhalla model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-small-E2E-qg_tweet_atomatic_original_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "YIwTcY9cgiZt",
    "outputId": "63290f44-55f3-4a67-b538-eed1b5d7dfcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 13:19:18.905180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [01:06<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "#using FINE-TUNED e2e small model\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  t5-small-E2E-qg-15-automatic \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-small-E2E-qg_tweet_atomatic_fine-tuned_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "r8cysPM6hrI6",
    "outputId": "108822d4-4aa6-473c-c169-6a4a6db6f256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.332793\n",
      "Bleu_2: 0.227647\n",
      "Bleu_3: 0.164830\n",
      "Bleu_4: 0.125187\n",
      "METEOR: 0.212231\n",
      "ROUGE_L: 0.351536\n",
      "CIDEr: 1.306929\n"
     ]
    }
   ],
   "source": [
    "#evaluate valhalla model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-small-E2E-qg_tweet_atomatic_fine-tuned_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5W77jJDnoLw"
   },
   "outputs": [],
   "source": [
    "DONE\n",
    "###############################\n",
    "# valhalla/t5-base-e2e-qg\n",
    "###############################\n",
    "%%capture loggingfine\n",
    "!python3 run_qg.py \\\n",
    "    --model_name_or_path valhalla/t5-base-e2e-qg \\\n",
    "    --model_type t5 \\\n",
    "    --tokenizer_name_or_path t5_qg_tokenizer \\\n",
    "    --output_dir t5-base-E2E-qg-15-automatic \\\n",
    "    --train_file_path data/train_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --num_train_epochs 15 \\\n",
    "    --seed 42 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluate_during_training \\\n",
    "    --logging_steps 100 \\\n",
    "    --logging_dir t5-base-E2E-qg-15-automatic-log\\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "rySFNikfg-Mg",
    "outputId": "38468a3c-6a02-4825-f7b0-a1717de92519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 13:20:49.440971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Downloading: 100% 792k/792k [00:00<00:00, 855kB/s]\n",
      "Downloading: 100% 31.0/31.0 [00:00<00:00, 18.3kB/s]\n",
      "Downloading: 100% 1.79k/1.79k [00:00<00:00, 1.08MB/s]\n",
      "Downloading: 100% 195/195 [00:00<00:00, 132kB/s]\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [02:46<00:00,  4.89s/it]\n"
     ]
    }
   ],
   "source": [
    "#using ORIGINAL e2e base model\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  valhalla/t5-base-e2e-qg\\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-base-E2E-qg_tweet_atomatic_original_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9nFqj_mFLbbE",
    "outputId": "0793b8c7-3bdb-4c1d-b470-d91c4411eaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.064967\n",
      "Bleu_2: 0.036087\n",
      "Bleu_3: 0.022533\n",
      "Bleu_4: 0.014561\n",
      "METEOR: 0.127423\n",
      "ROUGE_L: 0.100446\n",
      "CIDEr: 0.050683\n"
     ]
    }
   ],
   "source": [
    "#evaluate fine-tuned model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-base-E2E-qg_tweet_atomatic_original_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "Z6Pak61Vgxbq",
    "outputId": "db7ccea4-3d68-4b43-c57f-932f935f539a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-21 13:24:32.540222: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "  0% 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py:191: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return function(data_struct)\n",
      "100% 34/34 [02:07<00:00,  3.76s/it]\n"
     ]
    }
   ],
   "source": [
    "#using FINE-TUNED e2e base model\n",
    "!python eval.py \\\n",
    "    --model_name_or_path  t5-base-E2E-qg-15-automatic\\\n",
    "    --valid_file_path data/valid_data_qg_hl_t5_tweet_automatic.pt \\\n",
    "    --model_type t5 \\\n",
    "    --num_beams 4 \\\n",
    "    --max_decoding_length 32 \\\n",
    "    --output_path hypothesis_t5-base-E2E-qg_tweet_atomatic_fine-tuned_model.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "jp6-PavPUZf5",
    "outputId": "dcd004f2-8b03-4de0-ecec-69a0c0efc60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUsing data from /root/.cache/nlgeval\u001b[0m\n",
      "\u001b[32mIn case of broken downloads, remove the directory and run setup again.\u001b[0m\n",
      "Bleu_1: 0.396125\n",
      "Bleu_2: 0.285684\n",
      "Bleu_3: 0.214982\n",
      "Bleu_4: 0.166605\n",
      "METEOR: 0.247074\n",
      "ROUGE_L: 0.418279\n",
      "CIDEr: 1.788318\n"
     ]
    }
   ],
   "source": [
    "#evaluate fine-tuned model\n",
    "!nlg-eval --hypothesis=hypothesis_t5-base-E2E-qg_tweet_atomatic_fine-tuned_model.txt --references=data/tweet_dev_automatic_reference.txt --no-skipthoughts --no-glove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "2wlKrV4EzUnB",
    "outputId": "afc04620-a657-48ce-a7e3-e47646f27fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "references_tw.txt                       tweet_dev_automatic_reference.txt\n",
      "references.txt                          \u001b[0m\u001b[01;34mtweet_multitask\u001b[0m/\n",
      "\u001b[01;34msquad_multitask\u001b[0m/                        \u001b[01;34mtweet_multitask_old\u001b[0m/\n",
      "train_data_qg_hl_t5_sq.pt               valid_data_qg_hl_t5_sq.pt\n",
      "train_data_qg_hl_t5_tweet_automatic.pt  valid_data_qg_hl_t5_tweet_automatic.pt\n",
      "train_data_qg_hl_t5_tw.pt               valid_data_qg_hl_t5_tw.pt\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tweet_question_generation_NOW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
